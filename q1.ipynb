{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Packages and Libraries"
      ],
      "metadata": {
        "id": "cltmsb0FbSCq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LuuqJd38ZdIM"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
        "from torch.amp import GradScaler, autocast"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Patchify"
      ],
      "metadata": {
        "id": "EVpxVkribOF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=384):\n",
        "        super().__init__()\n",
        "        assert img_size % patch_size == 0\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6bmjgMqYZgzz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP"
      ],
      "metadata": {
        "id": "oq6FB1GubLls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        return self.drop(self.fc2(self.drop(self.act(self.fc1(x)))))"
      ],
      "metadata": {
        "id": "x68hbi3Ma02U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attention"
      ],
      "metadata": {
        "id": "DdWSbQNnbIuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=6, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
        "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        return self.proj_drop(self.proj(x))"
      ],
      "metadata": {
        "id": "Skxd_EMSa0zv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stochastic Depth"
      ],
      "metadata": {
        "id": "dnJ3qEhpbF7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StochasticDepth(nn.Module):\n",
        "    def __init__(self, p):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.p == 0.: return x\n",
        "        survival = 1. - self.p\n",
        "        mask = torch.rand(x.shape[0], 1, 1, device=x.device) < survival\n",
        "        return x * mask / survival"
      ],
      "metadata": {
        "id": "AmHmHXtIZhoI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Blocks"
      ],
      "metadata": {
        "id": "T154Ht3VbDtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
        "                              attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = StochasticDepth(drop_path)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, int(dim * mlp_ratio), drop=drop)\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "DglroT1oZhlg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vision Transformer"
      ],
      "metadata": {
        "id": "Yc3ykD9jbAeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, num_classes=10,\n",
        "                 embed_dim=384, depth=12, num_heads=6, mlp_ratio=4.,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, 3, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(drop_rate)\n",
        "\n",
        "        dpr = torch.linspace(0, drop_path_rate, depth)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True,\n",
        "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i])\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.pos_drop(x + self.pos_embed)\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])"
      ],
      "metadata": {
        "id": "S5lNoa0aZhin"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Learning Rate Scheduler"
      ],
      "metadata": {
        "id": "Yo6x0rVwa57k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupCosineLR(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, warmup_epochs, max_epochs, last_epoch=-1):\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.max_epochs = max_epochs\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "    def get_lr(self):\n",
        "        cur = self.last_epoch\n",
        "        if cur < self.warmup_epochs:\n",
        "            return [base_lr * (cur + 1) / self.warmup_epochs for base_lr in self.base_lrs]\n",
        "        t = (cur - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)\n",
        "        return [base_lr * 0.5 * (1 + math.cos(math.pi * t)) for base_lr in self.base_lrs]"
      ],
      "metadata": {
        "id": "XeBq3o8mZhce"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data"
      ],
      "metadata": {
        "id": "eJlPNzk1a2Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loaders(batch_size=128):\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                (0.247, 0.243, 0.261))\n",
        "    train_tfms = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm\n",
        "    ])\n",
        "    test_tfms = transforms.Compose([transforms.ToTensor(), norm])\n",
        "    train_set = datasets.CIFAR10(\"./data\", train=True, download=True, transform=train_tfms)\n",
        "    test_set = datasets.CIFAR10(\"./data\", train=False, download=True, transform=test_tfms)\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "KObMsx2TZhYi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "OEvOXCKHZo65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, test_loader, device, epochs=50, patience=10):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
        "    sched = WarmupCosineLR(opt, warmup_epochs=10, max_epochs=epochs)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, total_correct, n = 0.0, 0, 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            with autocast(device_type=device):\n",
        "                out = model(x)\n",
        "                loss = criterion(out, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "            batch_size = x.size(0)\n",
        "            total_loss += loss.item() * batch_size\n",
        "            total_correct += (out.argmax(1) == y).sum().item()\n",
        "            n += batch_size\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "        train_acc = total_correct / n * 100\n",
        "\n",
        "        model.eval()\n",
        "        test_correct, m = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                with autocast(device_type=device):\n",
        "                    out = model(x)\n",
        "                test_correct += (out.argmax(1) == y).sum().item()\n",
        "                m += x.size(0)\n",
        "        test_acc = test_correct / m * 100\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss {train_loss:.4f} | \"\n",
        "              f\"Train Acc {train_acc:.2f}% | Test Acc {test_acc:.2f}%\")\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_vit.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}. \"\n",
        "                      f\"Best accuracy: {best_acc:.2f}% (epoch {best_epoch+1})\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_vit.pth\"))\n",
        "    print(f\"Training finished. Best Test Accuracy: {best_acc:.2f}% at epoch {best_epoch+1}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "CRzTuCHCZhS3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run"
      ],
      "metadata": {
        "id": "qY9OOvqLZk8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learning rate = 3e-4, batch_size = 128"
      ],
      "metadata": {
        "id": "DdwVlT5Ks6yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_loader, test_loader = get_loaders()\n",
        "model = VisionTransformer().to(device)\n",
        "model = train(model, train_loader, test_loader, device, epochs=1000, patience=10)"
      ],
      "metadata": {
        "id": "8eRg0tt8JXCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d555202-4b40-473a-e82a-917dfdb8d4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [07:43<00:00, 368kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 | Train Loss 2.0803 | Train Acc 25.22% | Test Acc 35.31%\n",
            "Epoch 2/1000 | Train Loss 1.9388 | Train Acc 32.48% | Test Acc 40.05%\n",
            "Epoch 3/1000 | Train Loss 1.8187 | Train Acc 38.37% | Test Acc 47.59%\n",
            "Epoch 4/1000 | Train Loss 1.7353 | Train Acc 42.85% | Test Acc 51.34%\n",
            "Epoch 5/1000 | Train Loss 1.6714 | Train Acc 45.49% | Test Acc 51.77%\n",
            "Epoch 6/1000 | Train Loss 1.6294 | Train Acc 47.99% | Test Acc 57.32%\n",
            "Epoch 7/1000 | Train Loss 1.5836 | Train Acc 49.98% | Test Acc 59.99%\n",
            "Epoch 8/1000 | Train Loss 1.5596 | Train Acc 51.17% | Test Acc 61.11%\n",
            "Epoch 9/1000 | Train Loss 1.5318 | Train Acc 52.92% | Test Acc 63.19%\n",
            "Epoch 10/1000 | Train Loss 1.5088 | Train Acc 53.94% | Test Acc 62.39%\n",
            "Epoch 11/1000 | Train Loss 1.4745 | Train Acc 55.18% | Test Acc 65.29%\n",
            "Epoch 12/1000 | Train Loss 1.4437 | Train Acc 56.87% | Test Acc 65.67%\n",
            "Epoch 13/1000 | Train Loss 1.4082 | Train Acc 58.88% | Test Acc 66.95%\n",
            "Epoch 14/1000 | Train Loss 1.3821 | Train Acc 59.97% | Test Acc 69.03%\n",
            "Epoch 15/1000 | Train Loss 1.3537 | Train Acc 61.09% | Test Acc 68.51%\n",
            "Epoch 16/1000 | Train Loss 1.3342 | Train Acc 62.16% | Test Acc 70.71%\n",
            "Epoch 17/1000 | Train Loss 1.3101 | Train Acc 63.20% | Test Acc 72.47%\n",
            "Epoch 18/1000 | Train Loss 1.2886 | Train Acc 64.55% | Test Acc 72.77%\n",
            "Epoch 19/1000 | Train Loss 1.2705 | Train Acc 65.02% | Test Acc 72.24%\n",
            "Epoch 20/1000 | Train Loss 1.2503 | Train Acc 66.27% | Test Acc 74.10%\n",
            "Epoch 21/1000 | Train Loss 1.2323 | Train Acc 66.92% | Test Acc 74.78%\n",
            "Epoch 22/1000 | Train Loss 1.2172 | Train Acc 67.77% | Test Acc 76.40%\n",
            "Epoch 23/1000 | Train Loss 1.1975 | Train Acc 68.66% | Test Acc 76.71%\n",
            "Epoch 24/1000 | Train Loss 1.1833 | Train Acc 69.41% | Test Acc 77.43%\n",
            "Epoch 25/1000 | Train Loss 1.1683 | Train Acc 69.71% | Test Acc 77.03%\n",
            "Epoch 26/1000 | Train Loss 1.1569 | Train Acc 70.12% | Test Acc 77.54%\n",
            "Epoch 27/1000 | Train Loss 1.1418 | Train Acc 71.08% | Test Acc 78.68%\n",
            "Epoch 28/1000 | Train Loss 1.1320 | Train Acc 71.50% | Test Acc 78.59%\n",
            "Epoch 29/1000 | Train Loss 1.1175 | Train Acc 72.04% | Test Acc 79.89%\n",
            "Epoch 30/1000 | Train Loss 1.1049 | Train Acc 72.72% | Test Acc 80.27%\n",
            "Epoch 31/1000 | Train Loss 1.0958 | Train Acc 73.14% | Test Acc 78.82%\n",
            "Epoch 32/1000 | Train Loss 1.0802 | Train Acc 73.81% | Test Acc 81.04%\n",
            "Epoch 33/1000 | Train Loss 1.0659 | Train Acc 74.48% | Test Acc 79.29%\n",
            "Epoch 34/1000 | Train Loss 1.0603 | Train Acc 74.51% | Test Acc 82.11%\n",
            "Epoch 35/1000 | Train Loss 1.0460 | Train Acc 75.32% | Test Acc 82.04%\n",
            "Epoch 36/1000 | Train Loss 1.0342 | Train Acc 75.87% | Test Acc 82.53%\n",
            "Epoch 37/1000 | Train Loss 1.0269 | Train Acc 76.24% | Test Acc 82.35%\n",
            "Epoch 38/1000 | Train Loss 1.0189 | Train Acc 76.69% | Test Acc 83.16%\n",
            "Epoch 39/1000 | Train Loss 1.0059 | Train Acc 77.22% | Test Acc 82.53%\n",
            "Epoch 40/1000 | Train Loss 0.9988 | Train Acc 77.73% | Test Acc 82.95%\n",
            "Epoch 41/1000 | Train Loss 0.9918 | Train Acc 77.85% | Test Acc 83.32%\n",
            "Epoch 42/1000 | Train Loss 0.9793 | Train Acc 78.47% | Test Acc 83.58%\n",
            "Epoch 43/1000 | Train Loss 0.9704 | Train Acc 78.92% | Test Acc 83.30%\n",
            "Epoch 44/1000 | Train Loss 0.9632 | Train Acc 79.18% | Test Acc 84.77%\n",
            "Epoch 45/1000 | Train Loss 0.9555 | Train Acc 79.49% | Test Acc 84.52%\n",
            "Epoch 46/1000 | Train Loss 0.9485 | Train Acc 79.79% | Test Acc 85.16%\n",
            "Epoch 47/1000 | Train Loss 0.9401 | Train Acc 80.29% | Test Acc 85.37%\n",
            "Epoch 48/1000 | Train Loss 0.9356 | Train Acc 80.36% | Test Acc 84.83%\n",
            "Epoch 49/1000 | Train Loss 0.9274 | Train Acc 80.92% | Test Acc 85.44%\n",
            "Epoch 50/1000 | Train Loss 0.9184 | Train Acc 81.16% | Test Acc 85.38%\n",
            "Epoch 51/1000 | Train Loss 0.9112 | Train Acc 81.48% | Test Acc 85.71%\n",
            "Epoch 52/1000 | Train Loss 0.9082 | Train Acc 81.67% | Test Acc 85.64%\n",
            "Epoch 53/1000 | Train Loss 0.8971 | Train Acc 82.26% | Test Acc 86.01%\n",
            "Epoch 54/1000 | Train Loss 0.8935 | Train Acc 82.13% | Test Acc 86.32%\n",
            "Epoch 55/1000 | Train Loss 0.8875 | Train Acc 82.53% | Test Acc 85.64%\n",
            "Epoch 56/1000 | Train Loss 0.8868 | Train Acc 82.53% | Test Acc 86.60%\n",
            "Epoch 57/1000 | Train Loss 0.8783 | Train Acc 83.15% | Test Acc 86.68%\n",
            "Epoch 58/1000 | Train Loss 0.8684 | Train Acc 83.51% | Test Acc 87.22%\n",
            "Epoch 59/1000 | Train Loss 0.8620 | Train Acc 83.72% | Test Acc 86.90%\n",
            "Epoch 60/1000 | Train Loss 0.8630 | Train Acc 83.59% | Test Acc 86.41%\n",
            "Epoch 61/1000 | Train Loss 0.8572 | Train Acc 84.04% | Test Acc 86.96%\n",
            "Epoch 62/1000 | Train Loss 0.8541 | Train Acc 84.12% | Test Acc 86.96%\n",
            "Epoch 63/1000 | Train Loss 0.8478 | Train Acc 84.37% | Test Acc 87.06%\n",
            "Epoch 64/1000 | Train Loss 0.8431 | Train Acc 84.72% | Test Acc 87.54%\n",
            "Epoch 65/1000 | Train Loss 0.8417 | Train Acc 84.76% | Test Acc 85.05%\n",
            "Epoch 66/1000 | Train Loss 0.8380 | Train Acc 84.77% | Test Acc 87.74%\n",
            "Epoch 67/1000 | Train Loss 0.8284 | Train Acc 85.29% | Test Acc 86.76%\n",
            "Epoch 68/1000 | Train Loss 0.8246 | Train Acc 85.53% | Test Acc 87.05%\n",
            "Epoch 69/1000 | Train Loss 0.8254 | Train Acc 85.48% | Test Acc 87.48%\n",
            "Epoch 70/1000 | Train Loss 0.8198 | Train Acc 85.83% | Test Acc 87.51%\n",
            "Epoch 71/1000 | Train Loss 0.8147 | Train Acc 86.07% | Test Acc 87.23%\n",
            "Epoch 72/1000 | Train Loss 0.8120 | Train Acc 86.14% | Test Acc 87.65%\n",
            "Epoch 73/1000 | Train Loss 0.8079 | Train Acc 86.14% | Test Acc 88.30%\n",
            "Epoch 74/1000 | Train Loss 0.8021 | Train Acc 86.45% | Test Acc 88.26%\n",
            "Epoch 75/1000 | Train Loss 0.7968 | Train Acc 86.74% | Test Acc 87.46%\n",
            "Epoch 76/1000 | Train Loss 0.7951 | Train Acc 86.66% | Test Acc 87.68%\n",
            "Epoch 77/1000 | Train Loss 0.7950 | Train Acc 86.76% | Test Acc 88.40%\n",
            "Epoch 78/1000 | Train Loss 0.7892 | Train Acc 87.08% | Test Acc 88.22%\n",
            "Epoch 79/1000 | Train Loss 0.7913 | Train Acc 86.94% | Test Acc 88.67%\n",
            "Epoch 80/1000 | Train Loss 0.7882 | Train Acc 87.05% | Test Acc 87.96%\n",
            "Epoch 81/1000 | Train Loss 0.7830 | Train Acc 87.30% | Test Acc 88.15%\n",
            "Epoch 82/1000 | Train Loss 0.7758 | Train Acc 87.53% | Test Acc 88.41%\n",
            "Epoch 83/1000 | Train Loss 0.7735 | Train Acc 87.77% | Test Acc 87.98%\n",
            "Epoch 84/1000 | Train Loss 0.7743 | Train Acc 87.78% | Test Acc 87.88%\n",
            "Epoch 85/1000 | Train Loss 0.7704 | Train Acc 87.84% | Test Acc 88.72%\n",
            "Epoch 86/1000 | Train Loss 0.7673 | Train Acc 87.92% | Test Acc 89.05%\n",
            "Epoch 87/1000 | Train Loss 0.7662 | Train Acc 88.04% | Test Acc 88.78%\n",
            "Epoch 88/1000 | Train Loss 0.7644 | Train Acc 88.23% | Test Acc 88.67%\n",
            "Epoch 89/1000 | Train Loss 0.7604 | Train Acc 88.43% | Test Acc 88.74%\n",
            "Epoch 90/1000 | Train Loss 0.7606 | Train Acc 88.34% | Test Acc 88.92%\n",
            "Epoch 91/1000 | Train Loss 0.7600 | Train Acc 88.40% | Test Acc 88.72%\n",
            "Epoch 92/1000 | Train Loss 0.7557 | Train Acc 88.63% | Test Acc 88.37%\n",
            "Epoch 93/1000 | Train Loss 0.7498 | Train Acc 88.67% | Test Acc 88.98%\n",
            "Epoch 94/1000 | Train Loss 0.7545 | Train Acc 88.56% | Test Acc 88.85%\n",
            "Epoch 95/1000 | Train Loss 0.7474 | Train Acc 88.97% | Test Acc 89.04%\n",
            "Epoch 96/1000 | Train Loss 0.7479 | Train Acc 88.96% | Test Acc 88.25%\n",
            "Early stopping at epoch 96. Best accuracy: 89.05% (epoch 86)\n",
            "Training finished. Best Test Accuracy: 89.05% at epoch 86\n"
          ]
        }
      ]
    }
  ]
}