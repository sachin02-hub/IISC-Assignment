# Q1.ipynb

## Paper

The paper titled "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" presented at the ICLR 2021 by Dosovitskiy et al. lays a foundation to prove that Vision Transformers can replace CNN's for tasks like classifications.

Vision Transformer (ViT) applies a pure Transformer directly to image patches instead of using convolutions. Patches are generated by dividing with patch_size thereby converting the image into a number of smaller parts.
These patches are treated like tokens in an NLP.

The results of the paper proves that, When ViT is pre-trained on large datasets ,it surpasses top CNNs on image classification.
The ViT models achieve maximum accuracy with 2–4× less compute and strong transfer performance when compared with CNN's.

## Results
There is no particular prerequisites to run this notebook in Colab.

Click Run All button at the top will suffice.

The model architecture that delivered the best accuracy:

```
    img_size=32,

    patch_size=4,
    
    num_classes=10,
    
    embed_dim=192,
    
    depth=8,
    
    num_heads=3,
    
    mlp_ratio=4.0,
    
    drop_rate=0.1,
    
    attn_drop_rate=0.1,
    
    drop_path_rate=0.05
```

The Best Classification Accuracy obtained : 
    
#Q2.ipynb
I have removed the output of a cell in the notebook since it uses widget for selection (Video Masking with Manual Object Selection)
Github does not support interactive widget display, so if the output was present, Github displays invalid notebook if u try to open q2.ipynb
